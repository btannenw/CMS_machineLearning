{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has three hidden layers !!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Matplotlib is a matlab like plotting library\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "# SciKitLearn is a useful machine learning utilities library\n",
    "import sklearn\n",
    "# The sklearn dataset module helps generating |datasets\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset\n",
    "# import data\n",
    "from DataExtraction import dataNoMass as data\n",
    "from DataExtraction import dataWithP2\n",
    "from DataExtraction import dataWithP2E2 \n",
    "from DataExtraction import dataWithMass \n",
    "#from DataExtraction import p2E2 as data\n",
    "from DataExtraction import p2NegE2 \n",
    "#from DataExtraction import labels\n",
    "from DataExtraction import labels2D as labels\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data \n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, train_size=0.5, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any other data manipulations/printing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define softmax\n",
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss functions\n",
    "\n",
    "# softmax loss\n",
    "def softmax_loss(y,y_hat):\n",
    "    # clipping value \n",
    "    minval = 0.000000000001\n",
    "    # number of samples\n",
    "    m = y.shape[0]\n",
    "    # loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula \n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss\n",
    "\n",
    "# crossentropy loss\n",
    "def crossEntropy_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    if y.all() == 1:\n",
    "        return -1/m * np.sum(np.log(y_hat))\n",
    "    else:\n",
    "        return -1/m * np.sum(np.log(1 - y_hat))\n",
    "\n",
    "# mse loss\n",
    "def mse_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    return np.sum((y_hat - y)**2) / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define derivatives\n",
    "\n",
    "# loss derivative\n",
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "# tanh derivative\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propogation\n",
    "def forward_prop(model, a0):\n",
    "    \n",
    "    #Start Forward Propagation\n",
    "    \n",
    "    # Load parameters from model (1)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + our bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    # Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    # Third activation function\n",
    "    a3 = np.tanh(z3)\n",
    "    \n",
    "    # Fourth linear step\n",
    "    z4 = a3.dot(W4) + b4\n",
    "    \n",
    "    # For the Third linear activation function we use the softmax function, \n",
    "    # either the sigmoid of softmax should be used for the last layer\n",
    "    a4 = softmax(z4)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3,'a4':a4,'z4':z4}\n",
    "    return cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propogation\n",
    "def backward_prop(model, cache, y):\n",
    "\n",
    "    # Load parameters from model (2)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1,a2,a3,a4 = cache['a0'],cache['a1'],cache['a2'],cache['a3'],cache['a4']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    #calculate loss derivative with respect to output\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz4 = loss_derivative(y=y,y_hat=a4)\n",
    "\n",
    "    # Calculate loss derivative with respect to third layer weights\n",
    "    dW4 = 1/m*(a3.T).dot(dz4) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to third layer bias\n",
    "    db4 = 1/m*np.sum(dz4, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer\n",
    "    dz3 = np.multiply(dz4.dot(W4.T) ,tanh_derivative(a3))\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*np.dot(a2.T, dz3)\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW4':dW4,'db4':db4, 'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PHASE\n",
    "# this takes in the number of nodes in each layer\n",
    "def initialize_parameters(input_dim, l1_dim, l2_dim, l3_dim, output_dim):\n",
    "    \n",
    "    # first layer weights\n",
    "    W1 = 2 * np.random.randn(input_dim, l1_dim) -1\n",
    "    # first layer bias\n",
    "    b1 = np.zeros((1,l1_dim))\n",
    "    \n",
    "    # second layer weights\n",
    "    W2 = 2 * np.random.randn(l1_dim, l2_dim) -1\n",
    "    # second layer bias\n",
    "    b2 = np.zeros((1, l2_dim))\n",
    "    \n",
    "    # third layer weights\n",
    "    W3 = 2 * np.random.randn(l2_dim, l3_dim) -1\n",
    "    # third layer bias\n",
    "    b3 = np.zeros((1, l3_dim))\n",
    "    \n",
    "    # fourth layer weights (output layer)\n",
    "    W4 = 2 * np.random.randn(l3_dim, output_dim)\n",
    "    # fourth layer bias (output layer)\n",
    "    b4 = np.zeros((1, output_dim))\n",
    "    \n",
    "    # package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3, 'W4':W4, 'b4':b4}\n",
    "    return model\n",
    "\n",
    "def update_parameters(model, grads, learning_rate):\n",
    "   # Load parameters from model (3)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    W4 -= learning_rate * grads['dW4']\n",
    "    b4 -= learning_rate * grads['db4']\n",
    "    \n",
    "    # store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3, 'W4':W4, 'b4':b4}\n",
    "    return model\n",
    "\n",
    "# predict\n",
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = c['a4']\n",
    "    # plotArr.append([x, y_hat]) #added to make plot\n",
    "    return y_hat\n",
    "\n",
    "# calculate accuracy\n",
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "\n",
    "# train\n",
    "# change numbner of epochs here\n",
    "def train(model,X_,y_,learning_rate, epochs=2001, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        a4 = cache['a4'] \n",
    "        thisLoss = mse_loss(y_,a4) # set loss function here\n",
    "        losses.append(thisLoss)\n",
    "        y_hat = predict(model,X_) # getting rid of this because it's wrong\n",
    "        y_true = y_.argmax(axis=1)\n",
    "        accur = accuracy_score(a4,train_labels)\n",
    "        train_accuracies.append(accur)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            placeholderVar = accuracy_score(a4, train_labels)\n",
    "            test_accuracy = accuracyOfModel(model, test_data, test_labels)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            test_num.append(i)\n",
    "        #Printing loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 300==0:\n",
    "            print('Loss after iteration',i,':',thisLoss)\n",
    "            print('Train Accuracy after iteration',i,':',accur*100,'%')\n",
    "            print('Test Accuracy after iteration',i,':',test_accuracy*100,'%')\n",
    "    return model\n",
    "    \n",
    "# TESTING PHASE\n",
    "# test the accuracy of any model\n",
    "def accuracyOfModel(_model, _testData, _testLabels):\n",
    "    y_pred = predict(_model,_testData) # make predictions on test data\n",
    "    y_true = _testLabels # get usable info from labels\n",
    "    return accuracy_score(y_pred, y_true)\n",
    "\n",
    "def accuracy_score(_outputNodes, _labels):\n",
    "    for i in range(len(_outputNodes)-1):\n",
    "        if _outputNodes[i][0]>.5:\n",
    "            _outputNodes[i]=[1,0]\n",
    "        else:\n",
    "            _outputNodes[i]=[0,1]\n",
    "    numWrong = np.count_nonzero(np.subtract(_outputNodes,_labels))/2\n",
    "    return (len(_outputNodes)-numWrong)/len(_outputNodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 0.9761980112433126\n",
      "Train Accuracy after iteration 0 : 50.092285909332254 %\n",
      "Test Accuracy after iteration 0 : 49.65234760183055 %\n",
      "Loss after iteration 300 : 0.500708010062033\n",
      "Train Accuracy after iteration 300 : 50.072058860711486 %\n",
      "Test Accuracy after iteration 300 : 49.680159793684105 %\n",
      "Loss after iteration 600 : 0.5007070807347188\n",
      "Train Accuracy after iteration 600 : 50.0695304796339 %\n",
      "Test Accuracy after iteration 600 : 49.67763141260651 %\n",
      "Loss after iteration 900 : 0.5007043890908327\n",
      "Train Accuracy after iteration 900 : 50.0644737174787 %\n",
      "Test Accuracy after iteration 900 : 49.67510303152891 %\n",
      "Loss after iteration 1200 : 0.5006990795806183\n",
      "Train Accuracy after iteration 1200 : 50.0644737174787 %\n",
      "Test Accuracy after iteration 1200 : 49.67510303152891 %\n",
      "Loss after iteration 1500 : 0.5006975906994695\n",
      "Train Accuracy after iteration 1500 : 50.0644737174787 %\n",
      "Test Accuracy after iteration 1500 : 49.67510303152891 %\n",
      "Loss after iteration 1800 : 0.5006966217438716\n",
      "Train Accuracy after iteration 1800 : 50.06700209855629 %\n",
      "Test Accuracy after iteration 1800 : 49.67510303152891 %\n",
      "Loss after iteration 2100 : 0.5006952795753675\n",
      "Train Accuracy after iteration 2100 : 50.06700209855629 %\n",
      "Test Accuracy after iteration 2100 : 49.67257465045132 %\n"
     ]
    }
   ],
   "source": [
    "# plotArr = []\n",
    "losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "test_num = []\n",
    "np.random.seed(0)\n",
    "# This is what we return at the end\n",
    "model = initialize_parameters(input_dim=4, l1_dim=7, l2_dim=9, l3_dim=5, output_dim=2)\n",
    "model = train(model,train_data,train_labels,learning_rate=0.03,epochs=2101,print_loss=True) # original learning rate is 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Score')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(losses, label=\"Loss\")\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.scatter(test_num, test_accuracies, label=\"Test Accuracy\", s=16, color=\"green\")\n",
    "plt.legend()\n",
    "plt.title(\"Network Loss and Accuracy per Epoch (Pt Eta Phi E)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eulerspython",
   "language": "python",
   "name": "eulerspython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
