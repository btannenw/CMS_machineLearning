{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has three hidden layers !!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Matplotlib is a matlab like plotting library\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "# SciKitLearn is a useful machine learning utilities library\n",
    "import sklearn\n",
    "# The sklearn dataset module helps generating |datasets\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset\n",
    "# import data\n",
    "from DataExtraction import dataNoMass as data\n",
    "from DataExtraction import dataWithP2\n",
    "from DataExtraction import dataWithP2E2 \n",
    "from DataExtraction import dataWithMass \n",
    "#from DataExtraction import p2E2 as data\n",
    "from DataExtraction import p2NegE2 \n",
    "#from DataExtraction import labels\n",
    "from DataExtraction import labels2D as labels\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data \n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, train_size=0.5, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any other data manipulations/printing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define softmax\n",
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss functions\n",
    "\n",
    "# softmax loss\n",
    "def softmax_loss(y,y_hat):\n",
    "    # clipping value \n",
    "    minval = 0.000000000001\n",
    "    # number of samples\n",
    "    m = y.shape[0]\n",
    "    # loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula \n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss\n",
    "\n",
    "# crossentropy loss\n",
    "def crossEntropy_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    if y.all() == 1:\n",
    "        return -1/m * np.sum(np.log(y_hat))\n",
    "    else:\n",
    "        return -1/m * np.sum(np.log(1 - y_hat))\n",
    "\n",
    "# mse loss\n",
    "def mse_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    return np.sum((y_hat - y)**2) / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define derivatives\n",
    "\n",
    "# loss derivative\n",
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "# tanh derivative\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propogation\n",
    "def forward_prop(model, a0):\n",
    "    \n",
    "    #Start Forward Propagation\n",
    "    \n",
    "    # Load parameters from model (1)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + our bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    # Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    # Third activation function\n",
    "    a3 = np.tanh(z3)\n",
    "    \n",
    "    # Fourth linear step\n",
    "    z4 = a3.dot(W4) + b4\n",
    "    \n",
    "    # For the Third linear activation function we use the softmax function, \n",
    "    # either the sigmoid of softmax should be used for the last layer\n",
    "    a4 = softmax(z4)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3,'a4':a4,'z4':z4}\n",
    "    return cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propogation\n",
    "def backward_prop(model, cache, y):\n",
    "\n",
    "    # Load parameters from model (2)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1,a2,a3,a4 = cache['a0'],cache['a1'],cache['a2'],cache['a3'],cache['a4']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    #calculate loss derivative with respect to output\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz4 = loss_derivative(y=y,y_hat=a4)\n",
    "\n",
    "    # Calculate loss derivative with respect to third layer weights\n",
    "    dW4 = 1/m*(a3.T).dot(dz4) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to third layer bias\n",
    "    db4 = 1/m*np.sum(dz4, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer\n",
    "    dz3 = np.multiply(dz4.dot(W4.T) ,tanh_derivative(a3))\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*np.dot(a2.T, dz3)\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW4':dW4,'db4':db4, 'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PHASE\n",
    "# this takes in the number of nodes in each layer\n",
    "def initialize_parameters(input_dim, l1_dim, l2_dim, l3_dim, output_dim):\n",
    "    \n",
    "    # first layer weights\n",
    "    W1 = 2 * np.random.randn(input_dim, l1_dim) -1\n",
    "    # first layer bias\n",
    "    b1 = np.zeros((1,l1_dim))\n",
    "    \n",
    "    # second layer weights\n",
    "    W2 = 2 * np.random.randn(l1_dim, l2_dim) -1\n",
    "    # second layer bias\n",
    "    b2 = np.zeros((1, l2_dim))\n",
    "    \n",
    "    # third layer weights\n",
    "    W3 = 2 * np.random.randn(l2_dim, l3_dim) -1\n",
    "    # third layer bias\n",
    "    b3 = np.zeros((1, l3_dim))\n",
    "    \n",
    "    # fourth layer weights (output layer)\n",
    "    W4 = 2 * np.random.randn(l3_dim, output_dim)\n",
    "    # fourth layer bias (output layer)\n",
    "    b4 = np.zeros((1, output_dim))\n",
    "    \n",
    "    # package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3, 'W4':W4, 'b4':b4}\n",
    "    return model\n",
    "\n",
    "def update_parameters(model, grads, learning_rate):\n",
    "   # Load parameters from model (3)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    W4 -= learning_rate * grads['dW4']\n",
    "    b4 -= learning_rate * grads['db4']\n",
    "    \n",
    "    # store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3, 'W4':W4, 'b4':b4}\n",
    "    return model\n",
    "\n",
    "# predict\n",
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = c['a4']\n",
    "    # plotArr.append([x, y_hat]) #added to make plot\n",
    "    return y_hat\n",
    "\n",
    "# calculate accuracy\n",
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "\n",
    "# train\n",
    "# change numbner of epochs here\n",
    "def train(model,X_,y_,learning_rate, epochs=2001, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        a4 = cache['a4'] \n",
    "        thisLoss = mse_loss(y_,a4) # set loss function here\n",
    "        losses.append(thisLoss)\n",
    "        y_hat = predict(model,X_) # getting rid of this because it's wrong\n",
    "        y_true = y_.argmax(axis=1)\n",
    "        accur = accuracy_score(a4,train_labels)\n",
    "        train_accuracies.append(accur)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            placeholderVar = accuracy_score(a4, train_labels)\n",
    "            test_accuracy = accuracyOfModel(model, test_data, test_labels)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            test_num.append(i)\n",
    "        #Printing loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 300==0:\n",
    "            print('Loss after iteration',i,':',thisLoss)\n",
    "            print('Train Accuracy after iteration',i,':',accur*100,'%')\n",
    "            print('Test Accuracy after iteration',i,':',test_accuracy*100,'%')\n",
    "    return model\n",
    "    \n",
    "# TESTING PHASE\n",
    "# test the accuracy of any model\n",
    "def accuracyOfModel(_model, _testData, _testLabels):\n",
    "    y_pred = predict(_model,_testData) # make predictions on test data\n",
    "    y_true = _testLabels # get usable info from labels\n",
    "    return accuracy_score(y_pred, y_true)\n",
    "\n",
    "def accuracy_score(_outputNodes, _labels):\n",
    "    for i in range(len(_outputNodes)-1):\n",
    "        if _outputNodes[i][0]>.5:\n",
    "            _outputNodes[i]=[1,0]\n",
    "        else:\n",
    "            _outputNodes[i]=[0,1]\n",
    "    numWrong = np.count_nonzero(np.subtract(_outputNodes,_labels))/2\n",
    "    return (len(_outputNodes)-numWrong)/len(_outputNodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 0.9761980112433126\n",
      "Train Accuracy after iteration 0 : 50.092285909332254 %\n",
      "Test Accuracy after iteration 0 : 49.64476245859776 %\n",
      "Loss after iteration 300 : 0.5007567001047587\n",
      "Train Accuracy after iteration 300 : 50.0568885742459 %\n",
      "Test Accuracy after iteration 300 : 49.67763141260651 %\n",
      "Loss after iteration 600 : 0.5007086611259883\n",
      "Train Accuracy after iteration 600 : 50.072058860711486 %\n",
      "Test Accuracy after iteration 600 : 49.680159793684105 %\n",
      "Loss after iteration 900 : 0.5007088232964579\n",
      "Train Accuracy after iteration 900 : 50.072058860711486 %\n",
      "Test Accuracy after iteration 900 : 49.680159793684105 %\n",
      "Loss after iteration 1200 : 0.5007086970041389\n",
      "Train Accuracy after iteration 1200 : 50.072058860711486 %\n",
      "Test Accuracy after iteration 1200 : 49.680159793684105 %\n",
      "Loss after iteration 1500 : 0.5007083579335855\n",
      "Train Accuracy after iteration 1500 : 50.072058860711486 %\n",
      "Test Accuracy after iteration 1500 : 49.680159793684105 %\n",
      "Loss after iteration 1800 : 0.5007078377137631\n",
      "Train Accuracy after iteration 1800 : 50.0695304796339 %\n",
      "Test Accuracy after iteration 1800 : 49.680159793684105 %\n",
      "Loss after iteration 2100 : 0.5007071379367133\n",
      "Train Accuracy after iteration 2100 : 50.0695304796339 %\n",
      "Test Accuracy after iteration 2100 : 49.680159793684105 %\n"
     ]
    }
   ],
   "source": [
    "# plotArr = []\n",
    "losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "test_num = []\n",
    "np.random.seed(0)\n",
    "# This is what we return at the end\n",
    "model = initialize_parameters(input_dim=4, l1_dim=7, l2_dim=9, l3_dim=5, output_dim=2)\n",
    "model = train(model,train_data,train_labels,learning_rate=0.01,epochs=2101,print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Score')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses, label=\"Loss\")\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.scatter(test_num, test_accuracies, label=\"Test Accuracy\", s=16, color=\"green\")\n",
    "plt.legend()\n",
    "plt.title(\"Network Loss and Accuracy per Epoch (Pt Eta Phi E)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eulerspython",
   "language": "python",
   "name": "eulerspython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
