{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has three hidden layers !!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Matplotlib is a matlab like plotting library\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "# SciKitLearn is a useful machine learning utilities library\n",
    "import sklearn\n",
    "# The sklearn dataset module helps generating |datasets\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset\n",
    "# import data\n",
    "from DataExtraction import dataNoMass as data\n",
    "from DataExtraction import dataWithP2\n",
    "from DataExtraction import dataWithP2E2 \n",
    "from DataExtraction import dataWithMass \n",
    "#from DataExtraction import p2E2 as data\n",
    "from DataExtraction import p2NegE2 \n",
    "#from DataExtraction import labels\n",
    "from DataExtraction import labels2D as labels\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data \n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, train_size=0.5, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any other data manipulations/printing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define softmax\n",
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss functions\n",
    "\n",
    "# softmax loss\n",
    "def softmax_loss(y,y_hat):\n",
    "    # clipping value \n",
    "    minval = 0.000000000001\n",
    "    # number of samples\n",
    "    m = y.shape[0]\n",
    "    # loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula \n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss\n",
    "\n",
    "# crossentropy loss\n",
    "def crossEntropy_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    if y.all() == 1:\n",
    "        return -1/m * np.sum(np.log(y_hat))\n",
    "    else:\n",
    "        return -1/m * np.sum(np.log(1 - y_hat))\n",
    "\n",
    "# mse loss\n",
    "def mse_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    return np.sum((y_hat - y)**2) / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define derivatives\n",
    "\n",
    "# loss derivative\n",
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "# tanh derivative\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propogation\n",
    "def forward_prop(model, a0):\n",
    "    \n",
    "    #Start Forward Propagation\n",
    "    \n",
    "    # Load parameters from model (1)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + our bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    # Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    # Third activation function\n",
    "    a3 = np.tanh(z3)\n",
    "    \n",
    "    # Fourth linear step\n",
    "    z4 = a3.dot(W4) + b4\n",
    "    \n",
    "    # For the Third linear activation function we use the softmax function, \n",
    "    # either the sigmoid of softmax should be used for the last layer\n",
    "    a4 = softmax(z4)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3,'a4':a4,'z4':z4}\n",
    "    return cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propogation\n",
    "def backward_prop(model, cache, y):\n",
    "\n",
    "    # Load parameters from model (2)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1,a2,a3,a4 = cache['a0'],cache['a1'],cache['a2'],cache['a3'],cache['a4']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    #calculate loss derivative with respect to output\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz4 = loss_derivative(y=y,y_hat=a4)\n",
    "\n",
    "    # Calculate loss derivative with respect to third layer weights\n",
    "    dW4 = 1/m*(a3.T).dot(dz4) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to third layer bias\n",
    "    db4 = 1/m*np.sum(dz4, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer\n",
    "    dz3 = np.multiply(dz4.dot(W4.T) ,tanh_derivative(a3))\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*np.dot(a2.T, dz3)\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW4':dW4,'db4':db4, 'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PHASE\n",
    "# this takes in the number of nodes in each layer\n",
    "def initialize_parameters(input_dim, l1_dim, l2_dim, l3_dim, output_dim):\n",
    "    \n",
    "    # first layer weights\n",
    "    W1 = 2 * np.random.randn(input_dim, l1_dim) -1\n",
    "    # first layer bias\n",
    "    b1 = np.zeros((1,l1_dim))\n",
    "    \n",
    "    # second layer weights\n",
    "    W2 = 2 * np.random.randn(l1_dim, l2_dim) -1\n",
    "    # second layer bias\n",
    "    b2 = np.zeros((1, l2_dim))\n",
    "    \n",
    "    # third layer weights\n",
    "    W3 = 2 * np.random.randn(l2_dim, l3_dim) -1\n",
    "    # third layer bias\n",
    "    b3 = np.zeros((1, l3_dim))\n",
    "    \n",
    "    # fourth layer weights (output layer)\n",
    "    W4 = 2 * np.random.randn(l3_dim, output_dim)\n",
    "    # fourth layer bias (output layer)\n",
    "    b4 = np.zeros((1, output_dim))\n",
    "    \n",
    "    # package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3, 'W4':W4, 'b4':b4}\n",
    "    return model\n",
    "\n",
    "def update_parameters(model, grads, learning_rate):\n",
    "   # Load parameters from model (3)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    W4 -= learning_rate * grads['dW4']\n",
    "    b4 -= learning_rate * grads['db4']\n",
    "    \n",
    "    # store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3, 'W4':W4, 'b4':b4}\n",
    "    return model\n",
    "\n",
    "# predict\n",
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = c['a4']\n",
    "    # plotArr.append([x, y_hat]) #added to make plot\n",
    "    return y_hat\n",
    "\n",
    "# calculate accuracy\n",
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "\n",
    "# train\n",
    "# change numbner of epochs here\n",
    "def train(model,X_,y_,learning_rate, epochs=2001, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        a4 = cache['a4'] \n",
    "        thisLoss = mse_loss(y_,a4) # set loss function here\n",
    "        losses.append(thisLoss)\n",
    "        y_hat = predict(model,X_) # getting rid of this because it's wrong\n",
    "        y_true = y_.argmax(axis=1)\n",
    "        accur = accuracy_score(a4,train_labels)\n",
    "        train_accuracies.append(accur)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            placeholderVar = accuracy_score(a4, train_labels)\n",
    "            test_accuracy = accuracyOfModel(model, test_data, test_labels)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            test_num.append(i)\n",
    "        #Printing loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 300==0:\n",
    "            print('Loss after iteration',i,':',thisLoss)\n",
    "            print('Train Accuracy after iteration',i,':',accur*100,'%')\n",
    "            print('Test Accuracy after iteration',i,':',test_accuracy*100,'%')\n",
    "    return model\n",
    "    \n",
    "# TESTING PHASE\n",
    "# test the accuracy of any model\n",
    "def accuracyOfModel(_model, _testData, _testLabels):\n",
    "    y_pred = predict(_model,_testData) # make predictions on test data\n",
    "    y_true = _testLabels # get usable info from labels\n",
    "    return accuracy_score(y_pred, y_true)\n",
    "\n",
    "def accuracy_score(_outputNodes, _labels):\n",
    "    for i in range(len(_outputNodes)-1):\n",
    "        if _outputNodes[i][0]>.5:\n",
    "            _outputNodes[i]=[1,0]\n",
    "        else:\n",
    "            _outputNodes[i]=[0,1]\n",
    "    numWrong = np.count_nonzero(np.subtract(_outputNodes,_labels))/2\n",
    "    return (len(_outputNodes)-numWrong)/len(_outputNodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 0.9406414653414287\n",
      "Train Accuracy after iteration 0 : 50.0012641905388 %\n",
      "Test Accuracy after iteration 0 : 49.56385426411469 %\n",
      "Loss after iteration 300 : 0.5010083581586003\n",
      "Train Accuracy after iteration 300 : 50.0012641905388 %\n",
      "Test Accuracy after iteration 300 : 50.433617354807716 %\n",
      "Loss after iteration 600 : 0.5000025423897209\n",
      "Train Accuracy after iteration 600 : 50.0012641905388 %\n",
      "Test Accuracy after iteration 600 : 50.436145735885304 %\n",
      "Loss after iteration 900 : 0.5000000076049609\n",
      "Train Accuracy after iteration 900 : 50.0012641905388 %\n",
      "Test Accuracy after iteration 900 : 50.436145735885304 %\n",
      "Loss after iteration 1200 : 0.5000000008284098\n",
      "Train Accuracy after iteration 1200 : 50.0012641905388 %\n",
      "Test Accuracy after iteration 1200 : 49.56385426411469 %\n",
      "Loss after iteration 1500 : 0.5000000008073129\n",
      "Train Accuracy after iteration 1500 : 50.0012641905388 %\n",
      "Test Accuracy after iteration 1500 : 49.56385426411469 %\n",
      "Loss after iteration 1800 : 0.50000000080433\n",
      "Train Accuracy after iteration 1800 : 50.0012641905388 %\n",
      "Test Accuracy after iteration 1800 : 49.56385426411469 %\n",
      "Loss after iteration 2100 : 0.5000000008014004\n",
      "Train Accuracy after iteration 2100 : 50.0012641905388 %\n",
      "Test Accuracy after iteration 2100 : 49.56385426411469 %\n"
     ]
    }
   ],
   "source": [
    "# plotArr = []\n",
    "losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "test_num = []\n",
    "np.random.seed(0)\n",
    "learnRate = 0.01\n",
    "# This is what we return at the end\n",
    "model = initialize_parameters(input_dim=4, l1_dim=6, l2_dim=8, l3_dim=5, output_dim=2)\n",
    "model = train(model,train_data,train_labels,learning_rate=learnRate,epochs=2101,print_loss=True) # original learning rate is 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Score')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses, label=\"Loss\")\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.scatter(test_num, test_accuracies, label=\"Test Accuracy\", s=16, color=\"green\")\n",
    "plt.legend()\n",
    "plt.title(\"Network Loss and Accuracy per Epoch \\n (Pt Eta Phi E) with %1.3f Learning Rate\" %learnRate)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eulerspython",
   "language": "python",
   "name": "eulerspython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
