{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Matplotlib is a matlab like plotting library\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "# SciKitLearn is a useful machine learning utilities library\n",
    "import sklearn\n",
    "# The sklearn dataset module helps generating |datasets\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-36ecdd6be8fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# importing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mDataExtraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataNoMass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDataExtraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataWithP2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDataExtraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataWithP2E2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/LHCResearch/GroupGithub/leptonIdentificationNN/DataExtraction.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;31m# final export statements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0mdataNoMass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleadPt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadEta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadPhi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m \u001b[0mdataWithP2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDataWithP2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleadPt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadEta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadPhi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0mdataWithP2E2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDataWithP2E2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleadPt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadEta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadPhi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0mdataWithMass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDataWithMass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleadPt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadEta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadPhi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/LHCResearch/GroupGithub/leptonIdentificationNN/DataExtraction.py\u001b[0m in \u001b[0;36mgetDataWithP2\u001b[0;34m(_ptArr, _etaArr, _phiArr, _eArr, _isMuonArr, _pArr)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0mpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# importing dataset\n",
    "# import data\n",
    "from DataExtraction import dataNoMass\n",
    "from DataExtraction import dataWithP2\n",
    "from DataExtraction import dataWithP2E2 \n",
    "from DataExtraction import dataWithMass \n",
    "from DataExtraction import p2E2 as data\n",
    "from DataExtraction import p2NegE2 \n",
    "from DataExtraction import labels2D as labels\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data \n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, train_size=0.5, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# normalize test data\n",
    "# train_data[:,0] = train_data[:,0] / np.linalg.norm(train_data[:,0]) # normalize column 0\n",
    "# train_data[:,1] = train_data[:,1] / np.linalg.norm(train_data[:,1]) # normalize column 1\n",
    "#normalize train data\n",
    "# test_data[:,0] = test_data[:,0] / np.linalg.norm(test_data[:,0]) # normalize column 0\n",
    "# test_data[:,1] = test_data[:,1] / np.linalg.norm(test_data[:,1]) # normalize column 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avgE2 = np.mean(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data/avgE2\n",
    "test_data = test_data/avgE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messing with the number of training data points\n",
    "# train_data = train_data[0:9]\n",
    "# train_labels = train_labels[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define all our functions\n",
    "\n",
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "# loss functions\n",
    "def softmax_loss(y,y_hat):\n",
    "    # Clipping value\n",
    "    minval = 0.000000000001\n",
    "    # Number of samples\n",
    "    m = y.shape[0]\n",
    "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    #loss = -1/m * np.sum(y * np.log(y_hat))\n",
    "    return loss\n",
    "\n",
    "def crossEntropy_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    if y.all() == 1:\n",
    "        return -1/m * np.sum(np.log(y_hat))\n",
    "    else:\n",
    "        return -1/m * np.sum(np.log(1 - y_hat))\n",
    "\n",
    "def mse_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    return np.sum((y_hat - y)**2) / m\n",
    "    \n",
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n",
    "\n",
    "# This is the forward propagation function\n",
    "def forward_prop(model,a0):\n",
    "    \n",
    "    #Start Forward Propagation\n",
    "    \n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
    "    \n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + our bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    #Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return cache\n",
    "\n",
    "# This is the BACKWARD PROPAGATION function\n",
    "def backward_prop(model,cache,y):\n",
    "\n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    # Load forward propagation results\n",
    "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
    "\n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n",
    "\n",
    "#TRAINING PHASE\n",
    "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
    "    # First layer weights\n",
    "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
    "    \n",
    "    # First layer bias\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    \n",
    "    # Second layer weights\n",
    "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
    "    \n",
    "    # Second layer bias\n",
    "    b2 = np.zeros((1, nn_hdim))\n",
    "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
    "    b3 = np.zeros((1,nn_output_dim))\n",
    "    \n",
    "    \n",
    "    # Package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
    "    return model\n",
    "\n",
    "def update_parameters(model,grads,learning_rate):\n",
    "    # Load parameters\n",
    "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    \n",
    "    # load parameters into running lists\n",
    "    w00s.append(W1[0][0]) # modifies global list\n",
    "    w01s.append(W1[0][1]) # modifies global list\n",
    "    w02s.append(W1[0][2]) # modifies global list\n",
    "    w03s.append(W1[0][3]) # modifies global list\n",
    "    w04s.append(W1[0][4]) # modifies global list\n",
    "    \n",
    "    w10s.append(W1[1][0]) # modifies global list\n",
    "    w11s.append(W1[1][1]) # modifies global list\n",
    "    w12s.append(W1[1][2]) # modifies global list\n",
    "    w13s.append(W1[1][3]) # modifies global list\n",
    "    w14s.append(W1[1][4]) # modifies global list\n",
    "    \n",
    "    b0s.append(b1[0][0]) # modifies global list\n",
    "    b1s.append(b1[0][1]) # modifies global list\n",
    "    b2s.append(b1[0][2]) # modifies global list\n",
    "    b3s.append(b1[0][3]) # modifies global list\n",
    "    b4s.append(b1[0][4]) # modifies global list\n",
    "\n",
    "    # Store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "    return model\n",
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = c['a3']\n",
    "    # plotArr.append([x, y_hat]) #added to make plot\n",
    "    return y_hat\n",
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "def train(model,X_,y_,learning_rate, epochs=2001, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "        # it is at this point in the training that the weights get added to the lists\n",
    "    \n",
    "        a3 = cache['a3']\n",
    "        thisLoss = mse_loss(y_,a3) # set loss function here\n",
    "        losses.append(thisLoss) # modifies global list\n",
    "        y_hat = predict(model,X_) # getting rid of this because it's wrong\n",
    "        y_true = y_.argmax(axis=1)\n",
    "        accur = accuracy_score(a3,train_labels)\n",
    "        train_accuracies.append(accur) # modifies global list\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            placeholderVar = accuracy_score(a3, train_labels)\n",
    "            test_accuracy = accuracyOfModel(model, test_data, test_labels)\n",
    "            test_accuracies.append(test_accuracy) # modifies global list\n",
    "            test_num.append(i)\n",
    "        #Printing loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 300==0:\n",
    "            print('Loss after iteration',i,':',thisLoss)\n",
    "            print('Train Accuracy after iteration',i,':',accur*100,'%')\n",
    "            print('Test Accuracy after iteration',i,':',test_accuracy*100,'%')\n",
    "    return model\n",
    "\n",
    "# TESTING PHASE\n",
    "# test the accuracy of any model\n",
    "def accuracyOfModel(_model, _testData, _testLabels):\n",
    "    y_pred = predict(_model,_testData) # make predictions on test data\n",
    "    y_true = _testLabels # get usable info from labels\n",
    "    return accuracy_score(y_pred, y_true)\n",
    "\n",
    "def accuracy_score(_outputNodes, _labels):\n",
    "    for i in range(len(_outputNodes)-1):\n",
    "        if _outputNodes[i][0]>.5:\n",
    "            _outputNodes[i]=[1,0]\n",
    "        else:\n",
    "            _outputNodes[i]=[0,1]\n",
    "    numWrong = np.count_nonzero(np.subtract(_outputNodes,_labels))/2\n",
    "    return (len(_outputNodes)-numWrong)/len(_outputNodes)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare global lists\n",
    "losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "test_num = []\n",
    "\n",
    "w00s = []\n",
    "w01s = []\n",
    "w02s = []\n",
    "w03s = []\n",
    "w04s = []\n",
    "\n",
    "w10s = []\n",
    "w11s = []\n",
    "w12s = []\n",
    "w13s = []\n",
    "w14s = []\n",
    "\n",
    "b0s = []\n",
    "b1s = []\n",
    "b2s = []\n",
    "b3s = []\n",
    "b4s = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# declare global lists\n",
    "losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "test_num = []\n",
    "\n",
    "learnRate = 0.005 # set learning rate here\n",
    "np.random.seed(0)\n",
    "# This is what we return at the end\n",
    "model = initialize_parameters(nn_input_dim=2, nn_hdim= 6, nn_output_dim= 2)\n",
    "model = train(model,train_data,train_labels,learning_rate=learnRate,epochs=2101,print_loss=True) # original learning rate is 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, label=\"Loss\")\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.scatter(test_num, test_accuracies, label=\"Test Accuracy\", s=16, color=\"green\")\n",
    "#plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
    "plt.plot()\n",
    "plt.legend()\n",
    "plt.title(\"Network Loss and Accuracy per Epoch with %1.3f Learning Rate\" %learnRate)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot((w00s), label=\"Weight 0.0\")\n",
    "plt.plot((w01s), label=\"Weight 0.1\")\n",
    "plt.plot((w02s), label=\"Weight 0.2\")\n",
    "plt.plot((w03s), label=\"Weight 0.3\")\n",
    "plt.plot((w04s), label=\"Weight 0.4\")\n",
    "# plt.plot((w00s-(w00s[0])), label=\"Weight 0.0\")\n",
    "# plt.plot((w01s-(w01s[0])), label=\"Weight 0.1\")\n",
    "# plt.plot((w02s-(w02s[0])), label=\"Weight 0.2\")\n",
    "# plt.plot((w03s-(w03s[0])), label=\"Weight 0.3\")\n",
    "# plt.plot((w04s-(w04s[0])), label=\"Weight 0.4\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Weights From the First Input Node at Each Epoch\")\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot((w10s-(w10s[0])), label=\"Weight 1.0\")\n",
    "plt.plot((w11s-(w11s[0])), label=\"Weight 1.1\")\n",
    "plt.plot((w12s-(w12s[0])), label=\"Weight 1.2\")\n",
    "plt.plot((w13s-(w13s[0])), label=\"Weight 1.3\")\n",
    "plt.plot((w14s-(w14s[0])), label=\"Weight 1.4\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Change in Weights From the Second Input Node at Each Epoch\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(b0s, label=\"Bias 0\")\n",
    "plt.plot(b1s, label=\"Bias 1\")\n",
    "plt.plot(b2s, label=\"Bias 2\")\n",
    "plt.plot(b3s, label=\"Bias 3\")\n",
    "plt.plot(b4s, label=\"Bias 4\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Biases from Input Nodes at Each Epoch\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "print(\"Weight 1: \\n\", W1)\n",
    "print(\"Weight 2: \\n\", W2)\n",
    "print(\"Weight 3: \\n\", W3)\n",
    "print(\"Bias 1: \\n\", b1)\n",
    "print(\"Bias 2: \\n\", b2)\n",
    "print(\"Bias 3: \\n\", b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot of output node 2 vs output node 1\n",
    "# get weights and biases\n",
    "W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "\n",
    "diffArray = []\n",
    "\n",
    "plotX = []\n",
    "plotY = []\n",
    "\n",
    "inputArr = []\n",
    "outputArr = []\n",
    "for i in range(len(test_data)-1):\n",
    "    _a0 = test_data[i]\n",
    "    diffArray.append(_a0[1]-_a0[0])\n",
    "    inputArr.append(_a0[1])\n",
    "    _z1 = _a0.dot(W1) + b1\n",
    "    # Put it through the first activation function\n",
    "    _a1 = np.tanh(_z1)\n",
    "    # Second linear step\n",
    "    _z2 = _a1.dot(W2) + b2\n",
    "    # Second activation function\n",
    "    _a2 = np.tanh(_z2)\n",
    "    #Third linear step\n",
    "    _z3 = _a2.dot(W3) + b3\n",
    "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
    "    _a3 = softmax(_z3)\n",
    "    plotX.append(_a3[0][0])\n",
    "    plotY.append(_a3[0][1])\n",
    "plt.scatter(plotX, plotY)\n",
    "plt.title(\"Output node 2 vs Output node 1\")\n",
    "\n",
    "    # Calculate the point density\n",
    "#     xy = np.vstack([plotX,plotY])\n",
    "#     z = gaussian_kde(xy)(xy)\n",
    "\n",
    "#     fig, ax = plt.subplots()\n",
    "#     ax.scatter(x, y, c=z, s=100, edgecolor='')\n",
    "#     plt.show()\n",
    "\n",
    "#plt.hist(diffArray, bins=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 differences greater than .75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Difference in Electron and Muon Output Node Values (E-M)')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEICAYAAABbOlNNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHVBJREFUeJzt3Xm0XFWd9vHvI2EQBQJJjGSAoERb2gEwjaDLhpc4ANrCSyuCIpGOZjlro69G7V4gtAi2QkPjQtOgBhoZRIHY4MC4UJsgYZagEMKQhADXkEQwjUD7e//Yu+DsStW9davq1q0kz2etu+4Zdp3zq1N1znPOPnXrKiIwMzOrecFoF2BmZv3FwWBmZgUHg5mZFRwMZmZWcDCYmVnBwWBmZoW+DwZJ35b0z5Xxj0p6VNKTksZJepOke/P4IaNZa7vqn2MXl7ufpOXdXu6GRNI0SSFpzGjXYq0bzfeupAckvaXH69xS0mJJO47wej4p6eSh2o1qMOQX4H8kPSFpjaT/lvQRSc/VFREfiYgTcvvNgVOAt0XEiyNiFXA8cEYev3R0nklnqs9xuCRdJ+mpHIy1n590s758YN21m8vsF/k9+LSk8XXTb83Pe9oo1DRW0pmSHpG0TtKdko4exuO7elBtZXmSvp+3116VabtK6ukfSkmaK+n6BtPH59f51b2sZxjmANdHxEp4bns+Xbdf397swfk4EJJeVzf9kjx9vzzpP4D3S3rJYMX0wxXD30XENsDOwEnAF4Czm7SdCGwF3FWZtnPdeMs2orPIT+RgrP38XS9XvhFsx/uBI2ojkl4DbD0ahUjaAriK9L7eB9gO+H/ASZKOGY2ahuFx4F9GuYb/BN4oaZe66YcDd0bEb0ehplZ8BDi3btrX6/br1zV6YMU9wFG1EUnjSO+hgdq0iHgK+Gm1XSP9EAwARMTaiFgAvBeYVUv2nJz/IukVwO9z8zWSrpF0H/Ay4Cc5UbeUtJ2ksyWtlLQiP3azvKwPSvq1pFMlrQKOy9P/QdLdklZL+rmknWt15bT9SO6uWiPpW5JUmf/h/Ngn8qXgnnn6JEk/kjQg6X5Jn2r23GvPMQ/vJ2m5pM9Keiw/j5bPFgczWE2SNpP0JUn35edys6SplbOv2/M2fm+lxi9IegT4XmVbLJH0uKQFkia1uh3r6txL0g253UpJZ+QD5pDLys/jG5L+IGkp8I4WNs25lDvKLOCcupquk/ShyvgHJf2qMv5GSTdJWpt/v7HusSfk994Tkn6huiuUig8AOwHviYj7I+KZiPgZ8CngeEnbVrbBc1dxlf3kRaQdf5KeP9OcJOk4SRdLujDXcIsqZ5fDXV6T2ucDr5W0b6OZuY4F+f2xRNKHK/NemNe5WtJi4G8aPHbI/SkilgPX5O1YdRT5NZX0cqXjx6r8PjlP0tgmNT+3b+bx4uppiH1qL0mLJP1Rqfv7lCbr2Il0HLux0fxhOA94r/LxjnSycwnwdF276xhiv+ibYKiJiN8Ay4E3102/B/jrPDo2IvaPiJcDD5GuOl4cEX8Gvg88C+wK7AG8DfhQZVFvAJaSrj6+Kulg4EvAocAE4JfA+XVlvZP0Rn0tcBjwdgBJ7yGFy1HAtsC7gFVKXWE/AW4HJgMzgc9IenuLm+GlpDPFycBs4FuStm/xsQ21UNMxpDfSQfm5/AOwLiL+Ns9/Xd7GF1Zq3IF0ZjtH0v7A10jbZ0fgQeCCujIabscG/hf4R2A86YxnJvCxFpf14TxvD2AG8O7Btku2ENhW0qvyTnU46cyzJZJ2AC4HTgfGkbo7L1c6Y6t5H3A08BJgC+BzTRb3VuCnEfGnuuk/Il0t7zNYLflxBwIPV840H86zDwZ+SHrdfgBcqtQ92+7y6q0DTgS+2mT+BaR9exLpdTkxv28AjgVenn/eTgpnoKX3br35VIJB0iuB3fNzBhDpvToJeBUwlXySOBwt1HUacFpEbJuf10VNFvUaYGlEPDvcGuo8DCwmHfOgEoZ17gYGvfrou2DIHia9eYdF0kTSge0zEfGniHgMOJW0oz+37Ij494h4NiL+h3QJ97WIuDu/MCcCu6ty1QCcFBFrIuIh4FrSmwxS4Hw9Im6KZElEPEg6YE2IiOMj4umIWErq26vWMZhngOPz2eIVwJPAKwdpf3o+c679NLpfMVRNHwL+KSJ+n5/L7fkeTjN/AY6NiD/n7fh+4LsRcUsO6C8C+6jso2+2HQsRcXNELMyv0QPAd4D6s9BmyzoM+LeIWBYRj5MOAK2oXTW8lbTjrGjxcZDOvu6NiHNzzecDvwOqXXrfi4h78ra6iCbPnRSGK+sn5vfmH/L8dt0cERdHxDOk8NoK2LuD5TXyHWAnSQdWJ0qaCrwJ+EJEPBURtwFn8fyV2mHAVyPi8YhYRgrZmuHuT5cAEytXbUeRwnYAIO+nV+b37gBpWzS8yhnCUHU9A+wqaXxEPBkRC5ssZyzwRIPpn6vbr+e3UNM5wFGS/op0An1DgzZPkE48m+rXvuHJpP7K4doZ2BxYWemleAGwrNJmWYPHnCbpm5VpyjU8mMcfqcxbB7w4D08F7mtSxyRJayrTNiNdjbRiVd3ZQ3WdjXwqIs4aYplD1dTsuTQzkPsrayYBt9RGIuJJpe66ycADeXKz7VhQ6jY8hXTGvzXpfXpzXbNmy5pE+Ro/SGvOBa4HdqHxWdZgJjVYz4Ok517T0nMnHfzX+2SK0n2c8Xl+u57bLhHxl9wl0qxbqC0R8ed8YnIC5YF7EvB4RFQPgA+SXuPa/Gav27D2p4hYJ+mHpAPkDaSTls/W5ucTyNNIvRLbkI4Rq1t+kq3XNZv04ZjfSbof+EpE/FeD5azOddT7RkT8U/1ESd8GjsyjJ0bEiZXZPwa+Caxi/XsWNdsAa5vMA/owGCT9DWmH+tVQbRtYBvwZGD/IZVn9pySWkc5UzmtzfS9vMv3+iJjexjJHylA11Z5Lqzfn6rfjw6QdBYDcNz2O4Z1515wJ3AocERFPSPoMrXUJQTrbnloZ36mVB0XEg3nnPYi0Q9f7E+UN6ZdWhovnXlnvz1pZd52rSF0sL6rrTvp70nu7dta5rkE9tb7vZp8Eem675G6QKbn2dpfXzPdIHyI5tDLtYWAHSdtUwmEnnn9/1F63uyrzatrZn+YDl5IOlNuQunxqTiQ9p9dExONKH3M/o8lyBnvdB60rIu4Fjsjb+lDgYknjGnQT3gHsImlMK91JEfERUk9Ho3nrJP0U+CiNj02Qus+afsIJ+qgrSdK2kt5J6of8z4i4c7jLiPRRr18A38zLe0G+0TTYZeK3gS9K+utcx3b53kErziJd7r1eya65C+o3wBNKN2dfqHRD9NU59EbLUDWdBZwgaXp+Lq+t9JE/Sro5NpjzgaMl7S5pS9LOd2PuChqubYA/Ak/mS+KPDuOxFwGfkjQl35eZO4zHzgb2b7DjAtwGHCppa6WbtNXwuAJ4haT3SRoj6b3AbkCjs8OhnEs6IP9Q6W8wNs991qcDx0VE7UzvNuB9+XU8gLIr5FFgnKT67oLXSzo0X318hjJo2lleQ/ngdiwpHGrTlgH/DXxN0laSXkvahrV7OReR9sPtJU0BPllZZDv70y+BNcA84IKIqN6A3YbUPbtW0mTSp76auQ04SNIOkl5K2m4t1SXpSEkTIuIvuRZIXbD122s5sATYq35em74E7DvIvrcv6QMFTfVDMPxE0hOk9P0yqQuhk0/hHEW6ubeYdIl2MQ0uzWsi4hLgZOACSX8knTEf2Kx93WN/SLrR9gNSv92lwA4R8b+kG6C7kz4K+QfSgbelHasNZ6j8vHN9twst1HQKaef8BemgfDbwwjzvOGB+7uc8rFEBEXEV8M+km6QrSWcrrd5Tqfc50s3aJ0h9thcO3rzwH8DPSWdEt5DOGFsSEfdFxKIms08lfbrjUdLZ6HmVx60ibdvPki7hPw+8MyKG3e2T78+8hbQ/3Eh6LU4BvhwR/1pp+mnSPYw1pK6SSyvL+B0pqJfm16zWXXQZ6VN/q0k3Zw/N9xvaXd5gzmf9eyVHANNIVw+XkO5RXZXnfYXUfXQ/6T34XDdIO/tTRASpS3Bn1u8a/AqwJ6k75XIGf4+cS3ovPZDreu692EJdBwB3SXqS1HV1eL7H1Mh3WP+TVJ+v269bej9FxMMR0bDHRdJWpKviQe9XKPyPesw2epKOA3aNiCOHamu9l6+ybwVm5p6PkVrPJ4GpEfH5wdr13T0GM7NNTb5S3K0H6/n3Vtr1Q1eSmZn1EXclmZlZwVcMZmZW6Ot7DOPHj49p06aNdhlmZhuUm2+++Q8RMaHdx/d1MEybNo1Fi5p9etDMzBqR1Opf/DfkriQzMys4GMzMrOBgMDOzgoPBzMwKDgYzMys4GMzMrOBgMDOzgoPBzMwKDgYzMys4GGyjNm3u5aNdgtkGx8FgZmYFB4OZmRUcDGZmVnAwmJlZwcFgZmYFB4OZmRUcDGZmVnAwmJlZwcFgZmYFB4OZmRUcDGZmVnAwmJlZwcFgZmYFB4OZmRUcDGZmVnAwmJlZwcFgZmYFB4OZmRUcDGZmVnAwmJlZwcFgZmaFIYNB0nclPSbpt5VpO0i6UtK9+ff2eboknS5piaQ7JO1Zecys3P5eSbNG5umYmVmnWrli+D5wQN20ucDVETEduDqPAxwITM8/c4AzIQUJcCzwBmAv4NhamJiZWX8ZMhgi4nrg8brJBwPz8/B84JDK9HMiWQiMlbQj8Hbgyoh4PCJWA1eyftiYmVkfaPcew8SIWJmHHwEm5uHJwLJKu+V5WrPp65E0R9IiSYsGBgbaLM/MzNrV8c3niAggulBLbXnzImJGRMyYMGFCtxZrZmYtajcYHs1dROTfj+XpK4CplXZT8rRm083MrM+0GwwLgNoni2YBl1WmH5U/nbQ3sDZ3Of0ceJuk7fNN57flaWZm1mfGDNVA0vnAfsB4SctJny46CbhI0mzgQeCw3PwK4CBgCbAOOBogIh6XdAJwU253fETU39A2M7M+MGQwRMQRTWbNbNA2gI83Wc53ge8OqzozM+s5/+WzmZkVHAxmZlZwMJiZWcHBYGZmBQeDmZkVHAxmZlZwMJiZWcHBYGZmBQeDmZkVHAxmZlZwMJiZWcHBYGZmBQeDmZkVHAxmZlZwMJiZWcHBYGZmBQeDmZkVHAxmZlZwMJiZWcHBYGZmBQeDmZkVHAxmZlZwMJiZWcHBYGZmBQeDmZkVHAxmZlZwMJiZWaGjYJD0j5LukvRbSedL2krSLpJulLRE0oWStshtt8zjS/L8ad14AmZm1l1tB4OkycCngBkR8WpgM+Bw4GTg1IjYFVgNzM4PmQ2sztNPze3MzKzPdNqVNAZ4oaQxwNbASmB/4OI8fz5wSB4+OI+T58+UpA7Xb2ZmXdZ2METECuAbwEOkQFgL3AysiYhnc7PlwOQ8PBlYlh/7bG4/rn65kuZIWiRp0cDAQLvlmZlZmzrpStqedBWwCzAJeBFwQKcFRcS8iJgRETMmTJjQ6eLMzGyYOulKegtwf0QMRMQzwI+BNwFjc9cSwBRgRR5eAUwFyPO3A1Z1sH4zMxsBnQTDQ8DekrbO9wpmAouBa4F35zazgMvy8II8Tp5/TUREB+s3M7MR0Mk9hhtJN5FvAe7My5oHfAE4RtIS0j2Es/NDzgbG5enHAHM7qNvMzEbImKGbNBcRxwLH1k1eCuzVoO1TwHs6WZ+ZmY08/+WzmZkVHAxmZlZwMJiZWcHBYGZmBQeDmZkVHAxmZlZwMJiZWcHBYGZmBQeDmZkVHAxmZlZwMJiZWcHBYGZmBQeDbfKmzb18tEsw6ysOBjMzKzgYzMys4GAwM7OCg8HMzAoOBjMzKzgYzMys4GAwM7OCg8HMzAoOBjMzKzgYzMys4GAwM7OCg8HMzAoOBjMzKzgYzMys4GAwM7NCR8EgaaykiyX9TtLdkvaRtIOkKyXdm39vn9tK0umSlki6Q9Ke3XkKZmbWTZ1eMZwG/Cwi/gp4HXA3MBe4OiKmA1fncYADgen5Zw5wZofrNjOzEdB2MEjaDvhb4GyAiHg6ItYABwPzc7P5wCF5+GDgnEgWAmMl7dh25WZmNiI6uWLYBRgAvifpVklnSXoRMDEiVuY2jwAT8/BkYFnl8cvztIKkOZIWSVo0MDDQQXlmZtaOToJhDLAncGZE7AH8iee7jQCIiABiOAuNiHkRMSMiZkyYMKGD8szMrB2dBMNyYHlE3JjHLyYFxaO1LqL8+7E8fwUwtfL4KXmamZn1kbaDISIeAZZJemWeNBNYDCwAZuVps4DL8vAC4Kj86aS9gbWVLiczM+sTYzp8/CeB8yRtASwFjiaFzUWSZgMPAofltlcABwFLgHW5rZmZ9ZmOgiEibgNmNJg1s0HbAD7eyfrMzGzk+S+fzcys4GAwM7OCg8HMzAoOBjMzKzgYzFowbe7lo12CWc84GMzMrOBgMDOzgoPBzMwKDgYzMys4GMzMrOBgMDOzgoPBzMwKDgYzMys4GMzMrOBgMDOzgoPBzMwKDgYzMys4GMzMrOBgMDOzgoPBzMwKDgazJvw/GGxT5WAwM7OCg8HMzAoOBjMzKzgYzMys4GAwM7OCg8HMzAodB4OkzSTdKum/8vgukm6UtETShZK2yNO3zONL8vxpna7bzMy6rxtXDJ8G7q6MnwycGhG7AquB2Xn6bGB1nn5qbmdmZn2mo2CQNAV4B3BWHhewP3BxbjIfOCQPH5zHyfNn5vZmZtZHOr1i+Dfg88Bf8vg4YE1EPJvHlwOT8/BkYBlAnr82ty9ImiNpkaRFAwMDHZZnZmbD1XYwSHon8FhE3NzFeoiIeRExIyJmTJgwoZuLNjOzFnRyxfAm4F2SHgAuIHUhnQaMlTQmt5kCrMjDK4CpAHn+dsCqDtZv1pJufeeRvzvJNhVtB0NEfDEipkTENOBw4JqIeD9wLfDu3GwWcFkeXpDHyfOviYhod/1mZjYyRuLvGL4AHCNpCekewtl5+tnAuDz9GGDuCKzbzMw6NGboJkOLiOuA6/LwUmCvBm2eAt7TjfWZmdnI8V8+m5lZwcFgZmYFB4OZmRUcDGZmVnAwmJlZwcFgZmYFB4PZMPivn21T4GAwM7OCg8HMzAoOBjPcRWRW5WAw63MOLes1B4NZ5gOwWeJgMKtwOJg5GMzMrI6DwawP+crFRpODwczMCg4G2yQM9wzcZ+y2KXMwmJlZwcFgm4xuXQX4asI2dg4Gsz7lALLR4mAwM7OCg8Gsj/mqwUaDg8GsDdUDtg/etrFxMJjV8YHeNnUOBjMzKzgYzDrgqwvbGDkYzMys0HYwSJoq6VpJiyXdJenTefoOkq6UdG/+vX2eLkmnS1oi6Q5Je3brSZiZWfd0csXwLPDZiNgN2Bv4uKTdgLnA1RExHbg6jwMcCEzPP3OAMztYt1lbal0/7gIya67tYIiIlRFxSx5+ArgbmAwcDMzPzeYDh+Thg4FzIlkIjJW0Y9uVm5nZiOjKPQZJ04A9gBuBiRGxMs96BJiYhycDyyoPW56n1S9rjqRFkhYNDAx0ozwzMxuGjoNB0ouBHwGfiYg/VudFRAAxnOVFxLyImBERMyZMmNBpeWbr6VZ3kv/IzTZWHQWDpM1JoXBeRPw4T3601kWUfz+Wp68AplYePiVPMzOzPtLJp5IEnA3cHRGnVGYtAGbl4VnAZZXpR+VPJ+0NrK10OZmZWZ8Y08Fj3wR8ALhT0m152peAk4CLJM0GHgQOy/OuAA4ClgDrgKM7WLdZV7gLyGx9bQdDRPwKUJPZMxu0D+Dj7a7PzMx6w3/5bGZmBQeDmZkVHAxmXeR7FrYxcDCYdYlDwTYWDgYzMys4GMz6TCtXHr46sZHkYDAbgg/CtqlxMJiZWcHBYGZmBQeDWR9xt5X1AweD2QbAgWG95GAwM7OCg8GsT/iqwPqFg8Gsh5od/FsNhWlzL3eA2IhzMJj1WLsHdgeC9YqDwWyU+YBv/cbBYNZl1e4eH/RtQ+RgMBsF3bhX4NCxkeJgMOuBTm86m/WSg8FsED5w26bIwWA2QnyfwTZUDgazHnFA2IbCwWBmZgUHg5mZFRwMZmZWcDCYbcB838JGgoPBbAPncLBuczCYmVmh58Eg6QBJv5e0RNLcXq/fNm6b6tlz7Ss2NtXnb92liOjdyqTNgHuAtwLLgZuAIyJicaP2M2bMiEWLFvWsPtuw1A6CD5z0jmLcnt8mtmmSdHNEzGj38b2+YtgLWBIRSyPiaeAC4OAe12At6ucDbbU2nymvr3oF0ejbXludNtj0kap7NPj9U+r1FcO7gQMi4kN5/APAGyLiE5U2c4A5efSVwCrgDz0rsn3j6f86XWN3bAg1woZRp2vsjvoad46ICe0ubEzn9XRXRMwD5tXGJS3q5JKoVzaEOl1jd2wINcKGUadr7I5u19jrrqQVwNTK+JQ8zczM+kSvg+EmYLqkXSRtARwOLOhxDWZmNoiediVFxLOSPgH8HNgM+G5E3DXEw+YNMb9fbAh1usbu2BBqhA2jTtfYHV2tsac3n83MrP/5L5/NzKzgYDAzs0LfBYOkHSRdKene/Hv7Ju2+LukuSXdLOl2S+rDGnST9Ite4WNK0XtU4nDpz220lLZd0Rr/VKGl3STfk1/sOSe/tUW2Dfn2LpC0lXZjn39jr17fFGo/J7707JF0taede19hKnZV2fy8pJPX846Gt1CjpsLw975L0g36rMR9zrpV0a37ND2prRRHRVz/A14G5eXgucHKDNm8Efk26gb0ZcAOwXz/VmOddB7w1D78Y2LrftmWl7WnAD4Az+q1G4BXA9Dw8CVgJjB3hujYD7gNeBmwB3A7sVtfmY8C38/DhwIU93nat1Ph/au874KO9rrHVOnO7bYDrgYXAjH6rEZgO3Apsn8df0oc1zgM+mod3Ax5oZ119d8VA+oqM+Xl4PnBIgzYBbEXaOFsCmwOP9qS6ZMgaJe0GjImIKwEi4smIWNe7EoHWtiWSXg9MBH7Ro7qqhqwxIu6JiHvz8MPAY0Dbf9XZola+vqVa+8XAzF5eubZSY0RcW3nfLST97VCvtfpVOCcAJwNP9bK4rJUaPwx8KyJWA0TEY31YYwDb5uHtgIfbWVE/BsPEiFiZhx8hHbAKEXEDcC3pzHEl8POIuLt3JQ5dI+ksd42kH+fLun/NXyLYS0PWKekFwDeBz/WysIpWtuVzJO1FOiG4b4Trmgwsq4wvz9MatomIZ4G1wLgRrqvh+rNGNVbNBn46ohU1NmSdkvYEpkbEaH1pUSvb8hXAKyT9WtJCSQf0rLqklRqPA46UtBy4AvhkOysala/EkHQV8NIGs75cHYmIkLTe52kl7Qq8iufPfq6U9OaI+GW/1Ejatm8G9gAeAi4EPgic3a0au1Tnx4ArImL5SJ3sdqHG2nJ2BM4FZkXEX7pb5cZN0pHADGDf0a6lXj45OYW0f/SzMaTupP1Ix57rJb0mItaMalWlI4DvR8Q3Je0DnCvp1cPdX0YlGCLiLc3mSXpU0o4RsTIfCBpdrv1fYGFEPJkf81NgH6BrwdCFGpcDt0XE0vyYS4G96XIwdKHOfYA3S/oY6T7IFpKejIiu/a+MLtSIpG2By4EvR8TCbtU2iFa+vqXWZrmkMaRL91U9qK1+/TUNv2JG0ltIIbxvRPy5R7VVDVXnNsCrgevyyclLgQWS3hURvfre/Va25XLgxoh4Brhf0j2koLipNyW2VONs4ABIPSuStiJ9wd6wur36sStpATArD88CLmvQ5iFgX0ljJG1OOgvqZVdSKzXeBIyVVOsL3x9o+H8nRtCQdUbE+yNip4iYRupOOqebodCCIWtU+vqUS3JtF/eorla+vqVa+7uBayLf9euXGiXtAXwHeNco9InXDFpnRKyNiPERMS2/DxeS6u3lP2Np5fW+lHS1gKTxpK6lpX1W40PAzFzjq0j3YgeGvaZe3lVv8c77OOBq4F7gKmCHPH0GcFY8f3f+O6QwWAyc0m815vG3AncAdwLfB7boxzor7T9I7z+V1MrrfSTwDHBb5Wf3HtR2EOkfS91HulIBOJ500CLvdD8ElgC/AV7Wy23XYo1XkT6YUdtuC3pdYyt11rW9jh5/KqnFbSlSl9fivE8f3oc17kb6xObt+fV+Wzvr8VdimJlZoR+7kszMbBQ5GMzMrOBgMDOzgoPBzMwKDgYzMys4GMzMrOBgMDOzwv8HyLceFTGQRDwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "diffArr = []\n",
    "discardedVals = 0\n",
    "for x, y in zip(plotX, plotY):\n",
    "    if(abs(y-x)<.75):\n",
    "        diffArr.append(y-x)\n",
    "    else:\n",
    "        discardedVals += 1\n",
    "print(discardedVals, \"differences greater than .75\")\n",
    "plt.hist(diffArr, bins=1000)\n",
    "plt.title(\"Difference in Electron and Muon Output Node Values (E-M)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eulerspython",
   "language": "python",
   "name": "eulerspython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
