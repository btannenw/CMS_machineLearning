{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has three hidden layers !!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Matplotlib is a matlab like plotting library\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "# SciKitLearn is a useful machine learning utilities library\n",
    "import sklearn\n",
    "# The sklearn dataset module helps generating |datasets\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset\n",
    "# import data\n",
    "from DataExtraction import dataNoMass\n",
    "from DataExtraction import dataWithP2\n",
    "from DataExtraction import dataWithP2E2 \n",
    "from DataExtraction import dataWithMass \n",
    "#from DataExtraction import p2E2 as data\n",
    "from DataExtraction import p2NegE2 as data\n",
    "#from DataExtraction import labels\n",
    "from DataExtraction import labels2D as labels\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data \n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, train_size=0.5, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize/scale data here\n",
    "runSum = 0\n",
    "for e in train_data:\n",
    "    runSum+=e\n",
    "avgE2 = runSum/(len(train_data))\n",
    "\n",
    "train_data = train_data/avgE2\n",
    "test_data = test_data/avgE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any other data manipulations/printing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define softmax\n",
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss functions\n",
    "\n",
    "# softmax loss\n",
    "def softmax_loss(y,y_hat):\n",
    "    # clipping value \n",
    "    minval = 0.000000000001\n",
    "    # number of samples\n",
    "    m = y.shape[0]\n",
    "    # loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula \n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss\n",
    "\n",
    "# crossentropy loss\n",
    "def crossEntropy_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    if y.all() == 1:\n",
    "        return -1/m * np.sum(np.log(y_hat))\n",
    "    else:\n",
    "        return -1/m * np.sum(np.log(1 - y_hat))\n",
    "\n",
    "# mse loss\n",
    "def mse_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    return np.sum((y_hat - y)**2) / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define derivatives\n",
    "\n",
    "# loss derivative\n",
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "# tanh derivative\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propogation\n",
    "def forward_prop(model, a0):\n",
    "    \n",
    "    #Start Forward Propagation\n",
    "    \n",
    "    # Load parameters from model (1)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + our bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    # Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    # Third activation function\n",
    "    a3 = np.tanh(z3)\n",
    "    \n",
    "    # Fourth linear step\n",
    "    z4 = a3.dot(W4) + b4\n",
    "    \n",
    "    # For the Third linear activation function we use the softmax function, \n",
    "    # either the sigmoid of softmax should be used for the last layer\n",
    "    a4 = softmax(z4)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3,'a4':a4,'z4':z4}\n",
    "    return cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propogation\n",
    "def backward_prop(model, cache, y):\n",
    "\n",
    "    # Load parameters from model (2)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1,a2,a3,a4 = cache['a0'],cache['a1'],cache['a2'],cache['a3'],cache['a4']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    #calculate loss derivative with respect to output\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz4 = loss_derivative(y=y,y_hat=a4)\n",
    "\n",
    "    # Calculate loss derivative with respect to third layer weights\n",
    "    dW4 = 1/m*(a3.T).dot(dz4) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to third layer bias\n",
    "    db4 = 1/m*np.sum(dz4, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer\n",
    "    dz3 = np.multiply(dz4.dot(W4.T) ,tanh_derivative(a3))\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*np.dot(a2.T, dz3)\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW4':dW4,'db4':db4, 'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PHASE\n",
    "# this takes in the number of nodes in each layer\n",
    "def initialize_parameters(input_dim, l1_dim, l2_dim, l3_dim, output_dim):\n",
    "    \n",
    "    # first layer weights\n",
    "    W1 = 2 * np.random.randn(input_dim, l1_dim) -1\n",
    "    # first layer bias\n",
    "    b1 = np.zeros((1,l1_dim))\n",
    "    \n",
    "    # second layer weights\n",
    "    W2 = 2 * np.random.randn(l1_dim, l2_dim) -1\n",
    "    # second layer bias\n",
    "    b2 = np.zeros((1, l2_dim))\n",
    "    \n",
    "    # third layer weights\n",
    "    W3 = 2 * np.random.randn(l2_dim, l3_dim) -1\n",
    "    # third layer bias\n",
    "    b3 = np.zeros((1, l3_dim))\n",
    "    \n",
    "    # fourth layer weights (output layer)\n",
    "    W4 = 2 * np.random.randn(l3_dim, output_dim)\n",
    "    # fourth layer bias (output layer)\n",
    "    b4 = np.zeros((1, output_dim))\n",
    "    \n",
    "    # package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3, 'W4':W4, 'b4':b4}\n",
    "    return model\n",
    "\n",
    "def update_parameters(model, grads, learning_rate):\n",
    "   # Load parameters from model (3)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    W4 -= learning_rate * grads['dW4']\n",
    "    b4 -= learning_rate * grads['db4']\n",
    "    \n",
    "    # store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3, 'W4':W4, 'b4':b4}\n",
    "    return model\n",
    "\n",
    "# predict\n",
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = c['a4']\n",
    "    # plotArr.append([x, y_hat]) #added to make plot\n",
    "    return y_hat\n",
    "\n",
    "# calculate accuracy\n",
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "\n",
    "# train\n",
    "# change numbner of epochs here\n",
    "def train(model,X_,y_,learning_rate, epochs=2001, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        a4 = cache['a4'] \n",
    "        thisLoss = mse_loss(y_,a4) # set loss function here\n",
    "        losses.append(thisLoss)\n",
    "        y_hat = predict(model,X_) # getting rid of this because it's wrong\n",
    "        y_true = y_.argmax(axis=1)\n",
    "        accur = accuracy_score(a4,train_labels)\n",
    "        train_accuracies.append(accur)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            placeholderVar = accuracy_score(a4, train_labels)\n",
    "            test_accuracy = accuracyOfModel(model, test_data, test_labels)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            test_num.append(i)\n",
    "        #Printing loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 300==0:\n",
    "            print('Loss after iteration',i,':',thisLoss)\n",
    "            print('Train Accuracy after iteration',i,':',accur*100,'%')\n",
    "            print('Test Accuracy after iteration',i,':',test_accuracy*100,'%')\n",
    "    return model\n",
    "    \n",
    "# TESTING PHASE\n",
    "# test the accuracy of any model\n",
    "def accuracyOfModel(_model, _testData, _testLabels):\n",
    "    y_pred = predict(_model,_testData) # make predictions on test data\n",
    "    y_true = _testLabels # get usable info from labels\n",
    "    return accuracy_score(y_pred, y_true)\n",
    "\n",
    "def accuracy_score(_outputNodes, _labels):\n",
    "    for i in range(len(_outputNodes)-1):\n",
    "        if _outputNodes[i][0]>.5:\n",
    "            _outputNodes[i]=[1,0]\n",
    "        else:\n",
    "            _outputNodes[i]=[0,1]\n",
    "    numWrong = np.count_nonzero(np.subtract(_outputNodes,_labels))/2\n",
    "    return (len(_outputNodes)-numWrong)/len(_outputNodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 0.9477521662308858\n",
      "Train Accuracy after iteration 0 : 50.51452554929079 %\n",
      "Test Accuracy after iteration 0 : 49.10621728906981 %\n",
      "Loss after iteration 300 : 0.7187689341863094\n",
      "Train Accuracy after iteration 300 : 50.517053930368384 %\n",
      "Test Accuracy after iteration 300 : 49.58660969381305 %\n",
      "Loss after iteration 600 : 0.7187593689766117\n",
      "Train Accuracy after iteration 600 : 50.517053930368384 %\n",
      "Test Accuracy after iteration 600 : 49.58660969381305 %\n",
      "Loss after iteration 900 : 0.7187524766614255\n",
      "Train Accuracy after iteration 900 : 50.517053930368384 %\n",
      "Test Accuracy after iteration 900 : 49.58660969381305 %\n",
      "Loss after iteration 1200 : 0.7187441513970325\n",
      "Train Accuracy after iteration 1200 : 50.517053930368384 %\n",
      "Test Accuracy after iteration 1200 : 49.58660969381305 %\n",
      "Loss after iteration 1500 : 0.7187352598970839\n",
      "Train Accuracy after iteration 1500 : 50.517053930368384 %\n",
      "Test Accuracy after iteration 1500 : 49.58660969381305 %\n",
      "Loss after iteration 1800 : 0.7187271097579081\n",
      "Train Accuracy after iteration 1800 : 50.517053930368384 %\n",
      "Test Accuracy after iteration 1800 : 49.58660969381305 %\n",
      "Loss after iteration 2100 : 0.7187192028008782\n",
      "Train Accuracy after iteration 2100 : 50.517053930368384 %\n",
      "Test Accuracy after iteration 2100 : 49.58660969381305 %\n"
     ]
    }
   ],
   "source": [
    "# plotArr = []\n",
    "losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "test_num = []\n",
    "np.random.seed(0)\n",
    "# This is what we return at the end\n",
    "model = initialize_parameters(input_dim=2, l1_dim=5, l2_dim=5, l3_dim=5, output_dim=2)\n",
    "model = train(model,train_data,train_labels,learning_rate=0.81,epochs=2201,print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Score')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses, label=\"Loss\")\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.scatter(test_num, test_accuracies, label=\"Test Accuracy\", s=6, color=\"green\")\n",
    "plt.legend()\n",
    "plt.title(\"Network Loss and Accuracy per Epoch (P^2 -E^2)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eulerspython",
   "language": "python",
   "name": "eulerspython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
