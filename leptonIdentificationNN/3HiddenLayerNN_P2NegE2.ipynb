{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has three hidden layers !!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Matplotlib is a matlab like plotting library\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "# SciKitLearn is a useful machine learning utilities library\n",
    "import sklearn\n",
    "# The sklearn dataset module helps generating |datasets\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset\n",
    "# import data\n",
    "from DataExtraction import dataNoMass\n",
    "from DataExtraction import dataWithP2\n",
    "from DataExtraction import dataWithP2E2 \n",
    "from DataExtraction import dataWithMass \n",
    "#from DataExtraction import p2E2 as data\n",
    "from DataExtraction import p2NegE2 as data\n",
    "#from DataExtraction import labels\n",
    "from DataExtraction import labels2D as labels\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data \n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, train_size=0.5, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize/scale data here\n",
    "runSum = 0\n",
    "for e in train_data:\n",
    "    runSum+=e\n",
    "avgE2 = runSum/(len(train_data))\n",
    "\n",
    "train_data = train_data/avgE2\n",
    "test_data = test_data/avgE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any other data manipulations/printing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define softmax\n",
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss functions\n",
    "\n",
    "# softmax loss\n",
    "def softmax_loss(y,y_hat):\n",
    "    # clipping value \n",
    "    minval = 0.000000000001\n",
    "    # number of samples\n",
    "    m = y.shape[0]\n",
    "    # loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula \n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss\n",
    "\n",
    "# crossentropy loss\n",
    "def crossEntropy_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    if y.all() == 1:\n",
    "        return -1/m * np.sum(np.log(y_hat))\n",
    "    else:\n",
    "        return -1/m * np.sum(np.log(1 - y_hat))\n",
    "\n",
    "# mse loss\n",
    "def mse_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    return np.sum((y_hat - y)**2) / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define derivatives\n",
    "\n",
    "# loss derivative\n",
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "# tanh derivative\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propogation\n",
    "def forward_prop(model, a0):\n",
    "    \n",
    "    #Start Forward Propagation\n",
    "    \n",
    "    # Load parameters from model (1)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + our bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    # Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    # Third activation function\n",
    "    a3 = np.tanh(z3)\n",
    "    \n",
    "    # Fourth linear step\n",
    "    z4 = a3.dot(W4) + b4\n",
    "    \n",
    "    # For the Third linear activation function we use the softmax function, \n",
    "    # either the sigmoid of softmax should be used for the last layer\n",
    "    a4 = softmax(z4)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3,'a4':a4,'z4':z4}\n",
    "    return cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propogation\n",
    "def backward_prop(model, cache, y):\n",
    "\n",
    "    # Load parameters from model (2)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1,a2,a3,a4 = cache['a0'],cache['a1'],cache['a2'],cache['a3'],cache['a4']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    #calculate loss derivative with respect to output\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz4 = loss_derivative(y=y,y_hat=a4)\n",
    "\n",
    "    # Calculate loss derivative with respect to third layer weights\n",
    "    dW4 = 1/m*(a3.T).dot(dz4) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to third layer bias\n",
    "    db4 = 1/m*np.sum(dz4, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer\n",
    "    dz3 = np.multiply(dz4.dot(W4.T) ,tanh_derivative(a3))\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*np.dot(a2.T, dz3)\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW4':dW4,'db4':db4, 'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PHASE\n",
    "# this takes in the number of nodes in each layer\n",
    "def initialize_parameters(input_dim, l1_dim, l2_dim, l3_dim, output_dim):\n",
    "    \n",
    "    # first layer weights\n",
    "    W1 = 2 * np.random.randn(input_dim, l1_dim) -1\n",
    "    # first layer bias\n",
    "    b1 = np.zeros((1,l1_dim))\n",
    "    \n",
    "    # second layer weights\n",
    "    W2 = 2 * np.random.randn(l1_dim, l2_dim) -1\n",
    "    # second layer bias\n",
    "    b2 = np.zeros((1, l2_dim))\n",
    "    \n",
    "    # third layer weights\n",
    "    W3 = 2 * np.random.randn(l2_dim, l3_dim) -1\n",
    "    # third layer bias\n",
    "    b3 = np.zeros((1, l3_dim))\n",
    "    \n",
    "    # fourth layer weights (output layer)\n",
    "    W4 = 2 * np.random.randn(l3_dim, output_dim)\n",
    "    # fourth layer bias (output layer)\n",
    "    b4 = np.zeros((1, output_dim))\n",
    "    \n",
    "    # package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3, 'W4':W4, 'b4':b4}\n",
    "    return model\n",
    "\n",
    "def update_parameters(model, grads, learning_rate):\n",
    "   # Load parameters from model (3)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    W4 -= learning_rate * grads['dW4']\n",
    "    b4 -= learning_rate * grads['db4']\n",
    "    \n",
    "    # store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3, 'W4':W4, 'b4':b4}\n",
    "    return model\n",
    "\n",
    "# predict\n",
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = c['a4']\n",
    "    # plotArr.append([x, y_hat]) #added to make plot\n",
    "    return y_hat\n",
    "\n",
    "# calculate accuracy\n",
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "\n",
    "# train\n",
    "# change numbner of epochs here\n",
    "def train(model,X_,y_,learning_rate, epochs=2001, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        a4 = cache['a4'] \n",
    "        thisLoss = mse_loss(y_,a4) # set loss function here\n",
    "        losses.append(thisLoss)\n",
    "        y_hat = predict(model,X_) # getting rid of this because it's wrong\n",
    "        y_true = y_.argmax(axis=1)\n",
    "        accur = accuracy_score(a4,train_labels)\n",
    "        train_accuracies.append(accur)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            placeholderVar = accuracy_score(a4, train_labels)\n",
    "            test_accuracy = accuracyOfModel(model, test_data, test_labels)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            test_num.append(i)\n",
    "        #Printing loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 300==0:\n",
    "            print('Loss after iteration',i,':',thisLoss)\n",
    "            print('Train Accuracy after iteration',i,':',accur*100,'%')\n",
    "            print('Test Accuracy after iteration',i,':',test_accuracy*100,'%')\n",
    "    return model\n",
    "    \n",
    "# TESTING PHASE\n",
    "# test the accuracy of any model\n",
    "def accuracyOfModel(_model, _testData, _testLabels):\n",
    "    y_pred = predict(_model,_testData) # make predictions on test data\n",
    "    y_true = _testLabels # get usable info from labels\n",
    "    return accuracy_score(y_pred, y_true)\n",
    "\n",
    "def accuracy_score(_outputNodes, _labels):\n",
    "    for i in range(len(_outputNodes)-1):\n",
    "        if _outputNodes[i][0]>.5:\n",
    "            _outputNodes[i]=[1,0]\n",
    "        else:\n",
    "            _outputNodes[i]=[0,1]\n",
    "    numWrong = np.count_nonzero(np.subtract(_outputNodes,_labels))/2\n",
    "    return (len(_outputNodes)-numWrong)/len(_outputNodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 0.9477521662308858\n",
      "Train Accuracy after iteration 0 : 50.51452554929079 %\n",
      "Test Accuracy after iteration 0 : 50.883669186619805 %\n",
      "Loss after iteration 300 : 0.4982457468206001\n",
      "Train Accuracy after iteration 300 : 49.96081009329726 %\n",
      "Test Accuracy after iteration 300 : 50.23893201183283 %\n",
      "Loss after iteration 600 : 0.49794303978947135\n",
      "Train Accuracy after iteration 600 : 49.978508760840434 %\n",
      "Test Accuracy after iteration 600 : 50.26674420368637 %\n",
      "Loss after iteration 900 : 0.4978262467072445\n",
      "Train Accuracy after iteration 900 : 50.008849333771586 %\n",
      "Test Accuracy after iteration 900 : 50.309726682005504 %\n",
      "Loss after iteration 1200 : 0.49776365470253114\n",
      "Train Accuracy after iteration 1200 : 50.036661525625135 %\n",
      "Test Accuracy after iteration 1200 : 50.32489696847109 %\n",
      "Loss after iteration 1500 : 0.4977248880591599\n",
      "Train Accuracy after iteration 1500 : 50.0568885742459 %\n",
      "Test Accuracy after iteration 1500 : 50.35018077924705 %\n",
      "Loss after iteration 1800 : 0.4976992689740234\n",
      "Train Accuracy after iteration 1800 : 50.05941695532351 %\n",
      "Test Accuracy after iteration 1800 : 50.375464590023014 %\n",
      "Loss after iteration 2100 : 0.49768221136356333\n",
      "Train Accuracy after iteration 2100 : 50.0619453364011 %\n",
      "Test Accuracy after iteration 2100 : 50.39063487648858 %\n",
      "Loss after iteration 2400 : 0.497670183352809\n",
      "Train Accuracy after iteration 2400 : 50.06700209855629 %\n",
      "Test Accuracy after iteration 2400 : 50.40074840079897 %\n",
      "Loss after iteration 2700 : 0.497658304736641\n",
      "Train Accuracy after iteration 2700 : 50.07711562286667 %\n",
      "Test Accuracy after iteration 2700 : 50.40580516295417 %\n",
      "Loss after iteration 3000 : 0.4976453473172116\n",
      "Train Accuracy after iteration 3000 : 50.092285909332254 %\n",
      "Test Accuracy after iteration 3000 : 50.420975449419736 %\n",
      "Loss after iteration 3300 : 0.49763311100712265\n",
      "Train Accuracy after iteration 3300 : 50.10745619579783 %\n",
      "Test Accuracy after iteration 3300 : 50.453844403428484 %\n",
      "Loss after iteration 3600 : 0.49762240142128494\n",
      "Train Accuracy after iteration 3600 : 50.11504133903062 %\n",
      "Test Accuracy after iteration 3600 : 50.47407145204925 %\n",
      "Loss after iteration 3900 : 0.49761322276026376\n",
      "Train Accuracy after iteration 3900 : 50.14285353088418 %\n",
      "Test Accuracy after iteration 3900 : 50.49935526282522 %\n",
      "Loss after iteration 4200 : 0.4976053432485336\n",
      "Train Accuracy after iteration 4200 : 50.16308057950495 %\n",
      "Test Accuracy after iteration 4200 : 50.517053930368384 %\n",
      "Loss after iteration 4500 : 0.4975985175528277\n",
      "Train Accuracy after iteration 4500 : 50.17825086597052 %\n",
      "Test Accuracy after iteration 4500 : 50.53980936006675 %\n",
      "Loss after iteration 4800 : 0.49759254285932164\n",
      "Train Accuracy after iteration 4800 : 50.20606305782408 %\n",
      "Test Accuracy after iteration 4800 : 50.55750802760992 %\n",
      "Loss after iteration 5100 : 0.49758726148596993\n",
      "Train Accuracy after iteration 5100 : 50.22376172536725 %\n",
      "Test Accuracy after iteration 5100 : 50.567621551920304 %\n",
      "Loss after iteration 5400 : 0.49758255191943696\n",
      "Train Accuracy after iteration 5400 : 50.22629010644485 %\n",
      "Test Accuracy after iteration 5400 : 50.58532021946348 %\n",
      "Loss after iteration 5700 : 0.49757831975644506\n",
      "Train Accuracy after iteration 5700 : 50.23640363075522 %\n",
      "Test Accuracy after iteration 5700 : 50.60301888700665 %\n",
      "Loss after iteration 6000 : 0.49757449070011306\n",
      "Train Accuracy after iteration 6000 : 50.246517155065604 %\n",
      "Test Accuracy after iteration 6000 : 50.610604030239436 %\n",
      "Loss after iteration 6300 : 0.4975710054811939\n",
      "Train Accuracy after iteration 6300 : 50.26927258476398 %\n",
      "Test Accuracy after iteration 6300 : 50.630831078860204 %\n",
      "Loss after iteration 6600 : 0.4975678162121488\n",
      "Train Accuracy after iteration 6600 : 50.29202801446234 %\n",
      "Test Accuracy after iteration 6600 : 50.643472984248184 %\n",
      "Loss after iteration 6900 : 0.4975648837492651\n",
      "Train Accuracy after iteration 6900 : 50.30214153877273 %\n",
      "Test Accuracy after iteration 6900 : 50.66622841394655 %\n",
      "Loss after iteration 7200 : 0.4975621757582063\n",
      "Train Accuracy after iteration 7200 : 50.32742534954868 %\n",
      "Test Accuracy after iteration 7200 : 50.673813557179336 %\n",
      "Loss after iteration 7500 : 0.49755966527454065\n",
      "Train Accuracy after iteration 7500 : 50.34006725493666 %\n",
      "Test Accuracy after iteration 7500 : 50.68898384364492 %\n",
      "Loss after iteration 7800 : 0.4975573296173649\n",
      "Train Accuracy after iteration 7800 : 50.357765922479835 %\n",
      "Test Accuracy after iteration 7800 : 50.70415413011049 %\n",
      "Loss after iteration 8100 : 0.497555149558571\n",
      "Train Accuracy after iteration 8100 : 50.36029430355743 %\n",
      "Test Accuracy after iteration 8100 : 50.71426765442087 %\n",
      "Loss after iteration 8400 : 0.49755310867988123\n",
      "Train Accuracy after iteration 8400 : 50.36787944679022 %\n",
      "Test Accuracy after iteration 8400 : 50.73196632196405 %\n",
      "Loss after iteration 8700 : 0.4975511928696677\n",
      "Train Accuracy after iteration 8700 : 50.375464590023014 %\n",
      "Test Accuracy after iteration 8700 : 50.74460822735203 %\n",
      "Loss after iteration 9000 : 0.4975493899251409\n",
      "Train Accuracy after iteration 9000 : 50.38304973325579 %\n",
      "Test Accuracy after iteration 9000 : 50.74966498950722 %\n",
      "Loss after iteration 9300 : 0.4975476892348797\n",
      "Train Accuracy after iteration 9300 : 50.39063487648858 %\n",
      "Test Accuracy after iteration 9300 : 50.757250132740005 %\n",
      "Loss after iteration 9600 : 0.49754608152327573\n",
      "Train Accuracy after iteration 9600 : 50.39569163864378 %\n",
      "Test Accuracy after iteration 9600 : 50.76736365705039 %\n",
      "Loss after iteration 9900 : 0.4975445586431607\n",
      "Train Accuracy after iteration 9900 : 50.408333544031755 %\n",
      "Test Accuracy after iteration 9900 : 50.774948800283184 %\n",
      "Loss after iteration 10200 : 0.4975431134062749\n",
      "Train Accuracy after iteration 10200 : 50.41086192510935 %\n",
      "Test Accuracy after iteration 10200 : 50.79011908674875 %\n",
      "Loss after iteration 10500 : 0.4975417394437055\n",
      "Train Accuracy after iteration 10500 : 50.41339030618695 %\n",
      "Test Accuracy after iteration 10500 : 50.79264746782635 %\n",
      "Loss after iteration 10800 : 0.49754043109025287\n",
      "Train Accuracy after iteration 10800 : 50.41591868726455 %\n",
      "Test Accuracy after iteration 10800 : 50.80276099213673 %\n",
      "Loss after iteration 11100 : 0.4975391832880398\n",
      "Train Accuracy after iteration 11100 : 50.420975449419736 %\n",
      "Test Accuracy after iteration 11100 : 50.80781775429193 %\n",
      "Loss after iteration 11400 : 0.4975379915057028\n",
      "Train Accuracy after iteration 11400 : 50.436145735885304 %\n",
      "Test Accuracy after iteration 11400 : 50.81034613536952 %\n",
      "Loss after iteration 11700 : 0.4975368516702863\n",
      "Train Accuracy after iteration 11700 : 50.438674116962915 %\n",
      "Test Accuracy after iteration 11700 : 50.817931278602316 %\n",
      "Loss after iteration 12000 : 0.49753576010955397\n",
      "Train Accuracy after iteration 12000 : 50.45637278450607 %\n",
      "Test Accuracy after iteration 12000 : 50.817931278602316 %\n"
     ]
    }
   ],
   "source": [
    "# plotArr = []\n",
    "losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "test_num = []\n",
    "np.random.seed(0)\n",
    "# This is what we return at the end\n",
    "model = initialize_parameters(input_dim=2, l1_dim=5, l2_dim=5, l3_dim=5, output_dim=2)\n",
    "model = train(model,train_data,train_labels,learning_rate=0.01,epochs=12101,print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Score')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(losses, label=\"Loss\")\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.scatter(test_num, test_accuracies, label=\"Test Accuracy\", s=6, color=\"green\")\n",
    "plt.legend()\n",
    "plt.title(\"Network Loss and Accuracy per Epoch (P^2 -E^2)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eulerspython",
   "language": "python",
   "name": "eulerspython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
