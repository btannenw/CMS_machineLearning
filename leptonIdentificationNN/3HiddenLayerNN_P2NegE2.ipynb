{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has three hidden layers !!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Matplotlib is a matlab like plotting library\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "# SciKitLearn is a useful machine learning utilities library\n",
    "import sklearn\n",
    "# The sklearn dataset module helps generating |datasets\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset\n",
    "# import data\n",
    "from DataExtraction import dataNoMass\n",
    "from DataExtraction import dataWithP2\n",
    "from DataExtraction import dataWithP2E2 \n",
    "from DataExtraction import dataWithMass \n",
    "#from DataExtraction import p2E2 as data\n",
    "from DataExtraction import p2NegE2 as data\n",
    "#from DataExtraction import labels\n",
    "from DataExtraction import labels2D as labels\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data \n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, train_size=0.5, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize/scale data here\n",
    "runSum = 0\n",
    "for e in train_data:\n",
    "    runSum+=e\n",
    "avgE2 = runSum/(len(train_data))\n",
    "\n",
    "train_data = train_data/avgE2\n",
    "test_data = test_data/avgE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any other data manipulations/printing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define softmax\n",
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss functions\n",
    "\n",
    "# softmax loss\n",
    "def softmax_loss(y,y_hat):\n",
    "    # clipping value \n",
    "    minval = 0.000000000001\n",
    "    # number of samples\n",
    "    m = y.shape[0]\n",
    "    # loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula \n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss\n",
    "\n",
    "# crossentropy loss\n",
    "def crossEntropy_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    if y.all() == 1:\n",
    "        return -1/m * np.sum(np.log(y_hat))\n",
    "    else:\n",
    "        return -1/m * np.sum(np.log(1 - y_hat))\n",
    "\n",
    "# mse loss\n",
    "def mse_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    return np.sum((y_hat - y)**2) / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define derivatives\n",
    "\n",
    "# loss derivative\n",
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "# tanh derivative\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propogation\n",
    "def forward_prop(model, a0):\n",
    "    \n",
    "    #Start Forward Propagation\n",
    "    \n",
    "    # Load parameters from model (1)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + our bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    # Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    # Third activation function\n",
    "    a3 = np.tanh(z3)\n",
    "    \n",
    "    # Fourth linear step\n",
    "    z4 = a3.dot(W4) + b4\n",
    "    \n",
    "    # For the Third linear activation function we use the softmax function, \n",
    "    # either the sigmoid of softmax should be used for the last layer\n",
    "    a4 = softmax(z4)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3,'a4':a4,'z4':z4}\n",
    "    return cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propogation\n",
    "def backward_prop(model, cache, y):\n",
    "\n",
    "    # Load parameters from model (2)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1,a2,a3,a4 = cache['a0'],cache['a1'],cache['a2'],cache['a3'],cache['a4']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    #calculate loss derivative with respect to output\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz4 = loss_derivative(y=y,y_hat=a4)\n",
    "\n",
    "    # Calculate loss derivative with respect to third layer weights\n",
    "    dW4 = 1/m*(a3.T).dot(dz4) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to third layer bias\n",
    "    db4 = 1/m*np.sum(dz4, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer\n",
    "    dz3 = np.multiply(dz4.dot(W4.T) ,tanh_derivative(a3))\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*np.dot(a2.T, dz3)\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW4':dW4,'db4':db4, 'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PHASE\n",
    "# this takes in the number of nodes in each layer\n",
    "def initialize_parameters(input_dim, l1_dim, l2_dim, l3_dim, output_dim):\n",
    "    \n",
    "    # first layer weights\n",
    "    W1 = 2 * np.random.randn(input_dim, l1_dim) -1\n",
    "    # first layer bias\n",
    "    b1 = np.zeros((1,l1_dim))\n",
    "    \n",
    "    # second layer weights\n",
    "    W2 = 2 * np.random.randn(l1_dim, l2_dim) -1\n",
    "    # second layer bias\n",
    "    b2 = np.zeros((1, l2_dim))\n",
    "    \n",
    "    # third layer weights\n",
    "    W3 = 2 * np.random.randn(l2_dim, l3_dim) -1\n",
    "    # third layer bias\n",
    "    b3 = np.zeros((1, l3_dim))\n",
    "    \n",
    "    # fourth layer weights (output layer)\n",
    "    W4 = 2 * np.random.randn(l3_dim, output_dim)\n",
    "    # fourth layer bias (output layer)\n",
    "    b4 = np.zeros((1, output_dim))\n",
    "    \n",
    "    # package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3, 'W4':W4, 'b4':b4}\n",
    "    return model\n",
    "\n",
    "def update_parameters(model, grads, learning_rate):\n",
    "   # Load parameters from model (3)\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3'], model['W4'], model['b4']\n",
    "    \n",
    "    # update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    W4 -= learning_rate * grads['dW4']\n",
    "    b4 -= learning_rate * grads['db4']\n",
    "    \n",
    "    # store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3, 'W4':W4, 'b4':b4}\n",
    "    return model\n",
    "\n",
    "# predict\n",
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = c['a4']\n",
    "    # plotArr.append([x, y_hat]) #added to make plot\n",
    "    return y_hat\n",
    "\n",
    "# calculate accuracy\n",
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "\n",
    "# train\n",
    "# change numbner of epochs here\n",
    "def train(model,X_,y_,learning_rate, epochs=2001, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        a4 = cache['a4'] \n",
    "        thisLoss = mse_loss(y_,a4) # set loss function here\n",
    "        losses.append(thisLoss)\n",
    "        y_hat = predict(model,X_) # getting rid of this because it's wrong\n",
    "        y_true = y_.argmax(axis=1)\n",
    "        accur = accuracy_score(a4,train_labels)\n",
    "        train_accuracies.append(accur)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            placeholderVar = accuracy_score(a4, train_labels)\n",
    "            test_accuracy = accuracyOfModel(model, test_data, test_labels)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            test_num.append(i)\n",
    "        #Printing loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 300==0:\n",
    "            print('Loss after iteration',i,':',thisLoss)\n",
    "            print('Train Accuracy after iteration',i,':',accur*100,'%')\n",
    "            print('Test Accuracy after iteration',i,':',test_accuracy*100,'%')\n",
    "    return model\n",
    "    \n",
    "# TESTING PHASE\n",
    "# test the accuracy of any model\n",
    "def accuracyOfModel(_model, _testData, _testLabels):\n",
    "    y_pred = predict(_model,_testData) # make predictions on test data\n",
    "    y_true = _testLabels # get usable info from labels\n",
    "    return accuracy_score(y_pred, y_true)\n",
    "\n",
    "def accuracy_score(_outputNodes, _labels):\n",
    "    for i in range(len(_outputNodes)-1):\n",
    "        if _outputNodes[i][0]>.5:\n",
    "            _outputNodes[i]=[1,0]\n",
    "        else:\n",
    "            _outputNodes[i]=[0,1]\n",
    "    numWrong = np.count_nonzero(np.subtract(_outputNodes,_labels))/2\n",
    "    return (len(_outputNodes)-numWrong)/len(_outputNodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 0.9477521662308858\n",
      "Train Accuracy after iteration 0 : 50.51452554929079 %\n",
      "Test Accuracy after iteration 0 : 49.56385426411469 %\n",
      "Loss after iteration 300 : 0.9984292564822771\n",
      "Train Accuracy after iteration 300 : 49.996207428383606 %\n",
      "Test Accuracy after iteration 300 : 49.56385426411469 %\n",
      "Loss after iteration 600 : 0.9985109797150491\n",
      "Train Accuracy after iteration 600 : 49.996207428383606 %\n",
      "Test Accuracy after iteration 600 : 49.56385426411469 %\n",
      "Loss after iteration 900 : 0.9984520746306418\n",
      "Train Accuracy after iteration 900 : 49.996207428383606 %\n",
      "Test Accuracy after iteration 900 : 49.56385426411469 %\n",
      "Loss after iteration 1200 : 0.9983784702760401\n",
      "Train Accuracy after iteration 1200 : 49.996207428383606 %\n",
      "Test Accuracy after iteration 1200 : 49.56385426411469 %\n",
      "Loss after iteration 1500 : 0.9983336981184942\n",
      "Train Accuracy after iteration 1500 : 49.996207428383606 %\n",
      "Test Accuracy after iteration 1500 : 49.56385426411469 %\n",
      "Loss after iteration 1800 : 0.9983990194655025\n",
      "Train Accuracy after iteration 1800 : 49.996207428383606 %\n",
      "Test Accuracy after iteration 1800 : 49.56385426411469 %\n",
      "Loss after iteration 2100 : 0.9983609489522212\n",
      "Train Accuracy after iteration 2100 : 49.996207428383606 %\n",
      "Test Accuracy after iteration 2100 : 49.56385426411469 %\n"
     ]
    }
   ],
   "source": [
    "# plotArr = []\n",
    "losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "test_num = []\n",
    "\n",
    "learnRate = 2.43\n",
    "np.random.seed(0)\n",
    "# This is what we return at the end\n",
    "model = initialize_parameters(input_dim=2, l1_dim=5, l2_dim=5, l3_dim=5, output_dim=2)\n",
    "model = train(model,train_data,train_labels,learning_rate=learnRate,epochs=2101,print_loss=True) # original learning rate is 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Score')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses, label=\"Loss\")\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.scatter(test_num, test_accuracies, label=\"Test Accuracy\", s=6, color=\"green\")\n",
    "plt.legend()\n",
    "plt.title(\"Network Loss and Accuracy per Epoch \\n (P^2 -E^2) with %1.3f Learning Rate\" %learnRate)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eulerspython",
   "language": "python",
   "name": "eulerspython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
